# üöÄ Installation de Mixtral 8x7B quantis√© en local avec `llama.cpp`

## üì¶ 1. Pr√©requis syst√®me

### Sous Linux ou macOS :
```bash
sudo apt update && sudo apt install -y build-essential cmake python3 python3-pip git
```

### Sous macOS avec Homebrew :
```bash
brew install cmake python git
```

---

## üß∞ 2. Cloner et compiler `llama.cpp`

```bash
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp
make LLAMA_CUBLAS=1 # ‚ö†Ô∏è Si GPU NVIDIA CUDA (sinon fais juste `make`)
```

---

## üíæ 3. T√©l√©charger Mixtral 8x7B quantis√© `.gguf`

Rendez-vous sur : https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF

T√©l√©chargez par exemple :
```
Mixtral-8x7B-Instruct-v0.1.Q4_K_M.gguf
```
Et placez le fichier dans :
```
llama.cpp/models/mixtral/
```

---

## ‚ñ∂Ô∏è 4. Lancer le mod√®le en ligne de commande

```bash
cd llama.cpp
./main -m models/mixtral/Mixtral-8x7B-Instruct-v0.1.Q4_K_M.gguf -p "Bonjour, que peux-tu faire pour moi ?" -n 300
```

---

## üîå 5. Utilisation Python via `llama-cpp-python`

```bash
pip install llama-cpp-python
```

### Script minimal :
```python
from llama_cpp import Llama

llm = Llama(model_path="models/mixtral/Mixtral-8x7B-Instruct-v0.1.Q4_K_M.gguf")
res = llm("Explique la souverainet√© num√©rique.", max_tokens=200)
print(res["choices"][0]["text"])
```

---

## üß† Int√©gration avec IA_Florian
- Tu peux appeler ce mod√®le via un script `src/local_llm_call.py`
- Contextes s√©lectionn√©s depuis SQLite ou fichiers `.md`
- Envoi √† `llm()` puis r√©cup√©ration des r√©ponses √† r√©injecter

Souhaites-tu que je t‚Äô√©crive ce `local_llm_call.py` dans ton repo ?

